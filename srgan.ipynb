{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from PIL import Image\nfrom torch.utils import data\nimport glob\nimport torchvision.transforms as transforms\nimport numpy as np\n\nimport torch\nfrom torch import nn, optim\nfrom torchvision.models import vgg19\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt","metadata":{"id":"iXAF_5IcJ5Db","execution":{"iopub.status.busy":"2022-12-27T12:29:00.786989Z","iopub.execute_input":"2022-12-27T12:29:00.787354Z","iopub.status.idle":"2022-12-27T12:29:01.452688Z","shell.execute_reply.started":"2022-12-27T12:29:00.787319Z","shell.execute_reply":"2022-12-27T12:29:01.451735Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extract CelebA Dataset. [Source](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)","metadata":{"id":"dek3A9H5sSyE"}},{"cell_type":"code","source":"!pip install gdown\n!gdown 'https://drive.google.com/uc?id=13IvLXTmWc4hj4Tx4YpBW1LesN30UZusj'","metadata":{"execution":{"iopub.status.busy":"2022-12-27T12:29:01.454701Z","iopub.execute_input":"2022-12-27T12:29:01.455298Z","iopub.status.idle":"2022-12-27T12:29:32.636138Z","shell.execute_reply.started":"2022-12-27T12:29:01.455261Z","shell.execute_reply":"2022-12-27T12:29:32.634993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!unzip -oq celeb_dataset.zip","metadata":{"id":"BxapSmbgIPOO","execution":{"iopub.status.busy":"2022-12-27T12:29:32.637814Z","iopub.execute_input":"2022-12-27T12:29:32.638762Z","iopub.status.idle":"2022-12-27T12:29:54.620795Z","shell.execute_reply.started":"2022-12-27T12:29:32.638718Z","shell.execute_reply":"2022-12-27T12:29:54.619497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setup Dataset loading for PyTorch","metadata":{"id":"FUbbBljAshJF"}},{"cell_type":"code","source":"class Dataset(data.Dataset):\n    'Characterizes a dataset for PyTorch'\n    def __init__(self, data_folder, transform_lr, transform_hr, stage):\n        'Initialization'\n        file_list = glob.glob('{}/*'.format(data_folder))\n        n = len(file_list)\n        train_size = np.floor(n * 0.8).astype(int)\n        self.images = file_list[:train_size] if stage == 'train' else file_list[train_size:]\n\n        self.transform_lr = transforms.Compose(transform_lr)\n        self.transform_hr = transforms.Compose(transform_hr)\n\n    def __len__(self):\n        'Denotes the total number of samples'\n        # return len(self.images)\n        return 12800 # limit training dataset size to run in this notebook\n\n\n    def __getitem__(self, index):\n        'Generates one sample of data'\n        # Select sample\n\n        # Load data and get label\n        hr = Image.open(self.images[index])\n\n        return self.transform_lr(hr), self.transform_hr(hr)","metadata":{"id":"3Ti7ypq8JDtM","execution":{"iopub.status.busy":"2022-12-27T12:29:54.623993Z","iopub.execute_input":"2022-12-27T12:29:54.624672Z","iopub.status.idle":"2022-12-27T12:29:54.633519Z","shell.execute_reply.started":"2022-12-27T12:29:54.624632Z","shell.execute_reply":"2022-12-27T12:29:54.632284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## SRGAN Model Definition\n\n![Architecture](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-07-19_at_11.13.45_AM_zsF2pa7.png)\n\n[Source](https://arxiv.org/abs/1609.04802v5)","metadata":{"id":"ymbe08cutujA"}},{"cell_type":"code","source":"def weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm2d') != -1:\n        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, n_output=64, k_size=3, stride=1, padding=1):\n        \"\"\"\n        Residual block as defined in paper. In -> Conv -> BN -> PReLu -> Conv -> BN + In\n        \"\"\"\n        super(ResidualBlock, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(n_output, n_output, k_size, stride, padding),\n            nn.BatchNorm2d(n_output),\n            nn.PReLU(),\n            nn.Conv2d(n_output, n_output, k_size, stride, padding),\n            nn.BatchNorm2d(n_output),\n        )\n\n    def forward(self, x):\n        return x + self.model(x)\n\nclass ShuffleBlock(nn.Module):\n    def __init__(self, n_input, n_output, k_size=3, stride=1, padding=1):\n        \"\"\"\n        Shuffle block containing PixelShuffle layer. Conv -> PixelShuffle -> BN\n        \"\"\"\n        super(ShuffleBlock, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(n_input, n_output, k_size, stride, padding),  # N, 256, H, W\n            nn.PixelShuffle(2),  # N, 64, 2H, 2W\n            nn.PReLU(),\n        )\n        '''\n        Input: :math:`(N, C * upscale_factor^2, H, W)`\n        Output: :math:`(N, C, H * upscale_factor, W * upscale_factor)`\n        '''\n\n    def forward(self, x):\n        return self.model(x)\n\n\n# n_fmap = number of feature maps,\n# B = number of cascaded residual blocks.\nclass Generator(nn.Module):\n    def __init__(self, n_input=3, n_output=3, n_fmap=64, num_residual_blocks=16):\n        super(Generator, self).__init__()\n\n        self.l1 = nn.Sequential(\n            nn.Conv2d(n_input, n_fmap, 9, 1, 4),\n            nn.PReLU(),\n        )\n\n        # A cascaded of B residual blocks.\n        self.residual_blocks = []\n        for _ in range(num_residual_blocks):\n            self.residual_blocks.append(ResidualBlock(n_fmap))\n        self.residual_blocks = nn.Sequential(*self.residual_blocks)\n\n        self.l2 = nn.Sequential(\n            nn.Conv2d(n_fmap, n_fmap, 3, 1, 1),\n            nn.BatchNorm2d(n_fmap),\n        )\n\n        self.px = nn.Sequential(\n            ShuffleBlock(64, 256),\n            ShuffleBlock(64, 256),\n        )\n\n        self.conv_final = nn.Sequential(\n            nn.Conv2d(64, n_output, 9, 1, 4),\n            nn.Tanh(),\n        )\n\n\n    def forward(self, img_in):\n        out_1 = self.l1(img_in)\n        out_2 = self.residual_blocks(out_1)\n        out_3 = out_1 + self.l2(out_2)\n        out_4 = self.px(out_3)\n        return self.conv_final(out_4)\n\n\nclass Discriminator(nn.Module):\n    def __init__(self, lr_channels=3):\n        super(Discriminator, self).__init__()\n\n        def convblock(n_input, n_output, k_size=3, stride=1, padding=1, bn=True):\n            block = [nn.Conv2d(n_input, n_output, kernel_size=k_size, stride=stride, padding=padding, bias=False)]\n            if bn:\n                block.append(nn.BatchNorm2d(n_output))\n            block.append(nn.LeakyReLU(0.2, inplace=True))\n            return block\n\n        self.conv = nn.Sequential(\n            *convblock(lr_channels, 64, 3, 1, 1, bn=False),\n            *convblock(64, 64, 3, 2, 1),\n            *convblock(64, 128, 3, 1, 1),\n            *convblock(128, 128, 3, 2, 1),\n            *convblock(128, 256, 3, 1, 1),\n            *convblock(256, 256, 3, 2, 1),\n            *convblock(256, 512, 3, 1, 1),\n            *convblock(512, 512, 3, 2, 1),\n        )\n\n        self.fc = nn.Sequential(\n            nn.Linear(512 * 16 * 16, 1024),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Linear(1024, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, img):\n        out_1 = self.conv(img)\n        out_1 = out_1.view(img.size(0), -1)\n        out_2 = self.fc(out_1)\n        return out_2\n\n\nclass VGGFeatures(nn.Module):\n    def __init__(self):\n        super(VGGFeatures, self).__init__()\n        model = vgg19(pretrained=True)\n\n        children = list(model.features.children())\n        max_pool_indices = [index for index, m in enumerate(children) if isinstance(m, nn.MaxPool2d)]\n        target_features = children[:max_pool_indices[4]]\n        '''\n          We use vgg-5,4 which is the layer output after 5th conv \n          and right before the 4th max pool.\n        '''\n        self.features = nn.Sequential(*target_features)\n        for p in self.features.parameters():\n            p.requires_grad = False\n\n        '''\n        # VGG means and stdevs on pretrained imagenet\n        mean = -1 + Variable(torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n        std = 2*Variable(torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n\n        # This is for cuda compatibility.\n        self.register_buffer('mean', mean)\n        self.register_buffer('std', std)\n        '''\n\n    def forward(self, input):\n        # input = (input - self.mean) / self.std\n        output = self.features(input)\n        return output","metadata":{"id":"Cdl5JC7tt3dT","execution":{"iopub.status.busy":"2022-12-27T12:29:54.635389Z","iopub.execute_input":"2022-12-27T12:29:54.635720Z","iopub.status.idle":"2022-12-27T12:29:54.662393Z","shell.execute_reply.started":"2022-12-27T12:29:54.635686Z","shell.execute_reply":"2022-12-27T12:29:54.661445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set parameters","metadata":{"id":"g73wL5TktW9O"}},{"cell_type":"code","source":"class Config:\n  def __init__(self) -> None:\n      self.n_epochs = 10\n      self.batch_size = 16\n      self.lr = 0.0001\n      self.b1 = 0.9\n      self.b2 = 0.999\n      self.img_size = 256\n      self.data_folder = 'img_align_celeba'","metadata":{"id":"3PXFjNa3tKDs","execution":{"iopub.status.busy":"2022-12-27T12:29:54.664342Z","iopub.execute_input":"2022-12-27T12:29:54.664643Z","iopub.status.idle":"2022-12-27T12:29:54.676728Z","shell.execute_reply.started":"2022-12-27T12:29:54.664619Z","shell.execute_reply":"2022-12-27T12:29:54.675770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"opt = Config()\n\ntransform_image_hr = [\n    transforms.Resize((opt.img_size, opt.img_size), Image.BICUBIC),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n]\ntransform_image_lr = [\n    transforms.Resize((opt.img_size//4, opt.img_size//4), Image.BICUBIC),\n    transforms.ToTensor(),\n    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n]\ndata_train = Dataset(opt.data_folder, transform_image_lr, transform_image_hr, stage='train')\ndata_val = Dataset(opt.data_folder, transform_image_lr, transform_image_hr, stage='val')\n\nparams = {'batch_size': opt.batch_size, 'shuffle': True}\ndataloader_train = DataLoader(data_val, **params)\n\nparams = {'batch_size': 5, 'shuffle': True}\ndataloader_val = DataLoader(data_val, **params)\n\ncuda = torch.cuda.is_available()\nTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n\n# Loss\ngan_loss = nn.BCELoss()\ncontent_loss = nn.MSELoss()\n\ngenerator = Generator()\ndiscriminator = Discriminator()\nvgg = VGGFeatures()\n\noptimizer_D = optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\noptimizer_G = optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n\n# Loss record.\ng_losses = []\nd_losses = []\nepochs = []\nloss_legend = ['Discriminator', 'Generator']\n\nif cuda:\n    generator = generator.cuda()\n    discriminator = discriminator.cuda()\n    vgg = vgg.cuda()\n    gan_loss = gan_loss.cuda()\n    content_loss = content_loss.cuda()\n\n# Network weight init\ngenerator.apply(weights_init_normal)\ndiscriminator.apply(weights_init_normal)\n\nfrom tqdm import tqdm\nfor epoch in range(opt.n_epochs):\n    for i, (batch_lr, batch_hr) in tqdm(enumerate(dataloader_train), total=len(dataloader_train), desc='Step'):\n\n        real = Variable(Tensor(batch_lr.size(0), 1).fill_(1), requires_grad=False)\n        fake = Variable(Tensor(batch_lr.size(0), 1).fill_(0), requires_grad=False)\n\n        imgs_real_lr = Variable(batch_lr.type(Tensor))\n        imgs_real_hr = Variable(batch_hr.type(Tensor))\n\n        # == Discriminator update == #\n        optimizer_D.zero_grad()\n\n        imgs_fake_hr = Variable(generator(imgs_real_lr.detach()))\n\n        d_loss = gan_loss(discriminator(imgs_real_hr), real) + gan_loss(discriminator(imgs_fake_hr), fake)\n\n        d_loss.backward()\n        optimizer_D.step()\n\n        # == Generator update == #\n        imgs_fake_hr = generator(imgs_real_lr)\n\n        optimizer_G.zero_grad()\n\n        g_loss = (1/12.75) * content_loss(vgg(imgs_fake_hr), vgg(imgs_real_hr.detach())) + 1e-3 * gan_loss(discriminator(imgs_fake_hr), real)\n\n        g_loss.backward()\n        optimizer_G.step()\n\n        epochs.append(epoch + i/len(dataloader_train))\n        g_losses.append(g_loss.item())\n        d_losses.append(d_loss.item())","metadata":{"id":"LoFZIn9JKHRo","outputId":"d1b96ca3-bf24-4db3-c57e-659c4c372e96","execution":{"iopub.status.busy":"2022-12-27T16:03:59.402345Z","iopub.execute_input":"2022-12-27T16:03:59.402701Z","iopub.status.idle":"2022-12-27T18:41:37.047315Z","shell.execute_reply.started":"2022-12-27T16:03:59.402671Z","shell.execute_reply":"2022-12-27T18:41:37.046243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"g_losses_2 = [g_losses[int(i * len(g_losses) / 10)] for i in range(10)]\nd_losses_2 = [d_losses[int(i * len(d_losses) / 10)] for i in range(10)]","metadata":{"execution":{"iopub.status.busy":"2022-12-27T18:41:37.049222Z","iopub.execute_input":"2022-12-27T18:41:37.049689Z","iopub.status.idle":"2022-12-27T18:41:37.055573Z","shell.execute_reply.started":"2022-12-27T18:41:37.049648Z","shell.execute_reply":"2022-12-27T18:41:37.054445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(range(10), g_losses_2,  marker='o', label='Generator Loss')\nplt.plot(range(10), d_losses_2, marker='o', color = 'green', label='Discriminator Loss')\nplt.title('Generator & Discriminator Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(loc='best')\nplt.grid(True)","metadata":{"id":"BU7Alf0IJy17","execution":{"iopub.status.busy":"2022-12-27T18:41:37.057228Z","iopub.execute_input":"2022-12-27T18:41:37.058078Z","iopub.status.idle":"2022-12-27T18:41:37.303433Z","shell.execute_reply.started":"2022-12-27T18:41:37.058051Z","shell.execute_reply":"2022-12-27T18:41:37.302491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Generate 5 images from the validation set.\nbatch_lr, batch_hr = next(iter(dataloader_val))\n\nimgs_val_lr = Variable(batch_lr.type(Tensor))\nimgs_val_hr = Variable(batch_hr.type(Tensor))\nimgs_fake_hr = generator(imgs_val_lr).detach().data\n\n# For visualization purposes.\nimgs_val_lr = torch.nn.functional.upsample(imgs_val_lr, size=(imgs_fake_hr.size(2), imgs_fake_hr.size(3)), mode='bilinear')\n\nimgs_val_lr = imgs_val_lr.mul_(Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)).add_(Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\nimgs_val_hr = imgs_val_hr.mul_(Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1)).add_(Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\nimgs_fake_hr = imgs_fake_hr.add_(torch.abs(torch.min(imgs_fake_hr))).div_(torch.max(imgs_fake_hr)-torch.min(imgs_fake_hr))\nfake_val = torch.cat((imgs_val_lr, imgs_val_hr, imgs_fake_hr), dim=2)\n","metadata":{"id":"OfOJuMfCXLYl","execution":{"iopub.status.busy":"2022-12-27T18:48:19.794261Z","iopub.execute_input":"2022-12-27T18:48:19.794625Z","iopub.status.idle":"2022-12-27T18:48:19.867941Z","shell.execute_reply.started":"2022-12-27T18:48:19.794594Z","shell.execute_reply":"2022-12-27T18:48:19.867024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(nrows=len(batch_lr), ncols=3, figsize=(20,35))\nfor i in range(len(batch_lr)):\n    ax[i, 0].imshow(imgs_val_lr[i].detach().cpu().permute(1,2,0))\n    ax[i, 0].set_xticks([]), ax[i, 0].set_yticks([])\n    ax[i, 0].title.set_text('Original LR Image')\n\n    ax[i, 1].imshow(imgs_val_hr[i].detach().cpu().permute(1,2,0))\n    ax[i, 1].set_xticks([]), ax[i, 1].set_yticks([])\n    ax[i, 1].title.set_text('Original HR Image')\n\n    ax[i, 2].imshow(imgs_fake_hr[i].detach().cpu().permute(1,2,0))\n    ax[i, 2].set_xticks([]), ax[i, 2].set_yticks([])\n    ax[i, 2].title.set_text('SRGAN Generated Image')\n","metadata":{"id":"qwwol5wsKLo4","execution":{"iopub.status.busy":"2022-12-27T18:48:23.847892Z","iopub.execute_input":"2022-12-27T18:48:23.848478Z","iopub.status.idle":"2022-12-27T18:48:26.175112Z","shell.execute_reply.started":"2022-12-27T18:48:23.848442Z","shell.execute_reply":"2022-12-27T18:48:26.173905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = torch.randn(1, 3, 64, 64, requires_grad=True, dtype=torch.float32).to('cuda')\ntorch.onnx.export(generator,               # model being run\n                  x,                         # model input (or a tuple for multiple inputs)\n                  \"super_resolution.onnx\",   # where to save the model (can be a file or file-like object)\n                  export_params=True,        # store the trained parameter weights inside the model file\n                  opset_version=10,          # the ONNX version to export the model to\n                  do_constant_folding=True,  # whether to execute constant folding for optimization\n                  input_names = ['l1'],   # the model's input names\n                  output_names = ['conv_final'], # the model's output names\n                  # dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n                  #               'output' : {0 : 'batch_size'}}\n)","metadata":{"id":"5TCN0BiI7lOE","execution":{"iopub.status.busy":"2022-12-27T18:48:09.587568Z","iopub.execute_input":"2022-12-27T18:48:09.588074Z","iopub.status.idle":"2022-12-27T18:48:11.032846Z","shell.execute_reply.started":"2022-12-27T18:48:09.588033Z","shell.execute_reply":"2022-12-27T18:48:11.031747Z"},"trusted":true},"execution_count":null,"outputs":[]}]}